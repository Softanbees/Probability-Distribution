---
title: "DISTRIBUTION"
author: "SOFTANBEES TECHNOLOGIES PVT. LTD."

output: 
 html_document:
    toc: true
    toc_float: true
---

<h1> **1. Basic Probability Distributions in R**</h1> 
\
R comes with built-in implementations of many probability distributions. This document will show how to generate these distributions in R by focusing on making plots, and so give the reader an intuitive feel for what all the different R functions are actually calculating.
\
\
Each probability distribution in R is associated with four functions which follow a naming convention: the probability density function always begins with ‘d’, the cumulative distribution function always begins with ‘p’, the inverse cumulative distrobution (or quantile function) always beings with ‘q’, and a function that produces random variables always begins with ‘r’. Each function takes a single argument at which to evaluate the function followed by specific parameters that define the particular distribution function to evaluate.
\
\
In the following we will demonstrate usage for the density, disribution and quantile functions only. Each demonstration will include plots and simple examples.
\
\

![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table1.JPG)

\

\
<h1>**Probability Distribution Prerequisites** </h1> 
\
To understand probability distributions, it is important to understand variables. random variables, and some notation.
\

- A variable is a symbol (A, B, x, y, etc.) that can take on any of a specified set of values.\

- When the value of a variable is the outcome of a statistical experiment, that variable is a random variable.\
\
Generally, statisticians use a capital letter to represent a random variable and a lower-case letter, to represent one of its values. For example,

- X represents the random variable X.\

- P(X) represents the probability of X.\

- P(X = x) refers to the probability that the random variable X is equal to a particular value, denoted by x. As an example, P(X = 1) refers to the probability that the random variable X is equal to 1.\
\


<h2>1.1. What is a Probability Distribution?</h2> 
\

**"A probability distribution is a list of all of the possible outcomes of a random variable along with their corresponding probability values." **
\
\

An example will make clear the relationship between random variables and probability distributions. Suppose you flip a coin two times. This simple statistical experiment can have four possible outcomes: HH, HT, TH, and TT. 
\
Now, let the variable X represent the number of Heads that result from this experiment. The variable X can take on the values 0, 1, or 2. In this example, X is a random variable; because its value is determined by the outcome of a statistical experiment.

A probability distribution is a table or an equation that links each outcome of a statistical experiment with its probability of occurrence. Consider the coin flip experiment described above. The table below, which associates each outcome with its probability, is an example of a probability distribution.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table2.JPG)

\

The above table represents the probability distribution of the random variable X.


\

<h3> 1.1.1 Statistical Experiment</h3> 
\

**All statistical experiments have three things in common:**

- The experiment can have more than one possible outcome.\

- Each possible outcome can be specified in advance.\

- The outcome of the experiment depends on chance.\
\

A coin toss has all the attributes of a statistical experiment. There is more than one possible outcome. We can specify each possible outcome (i.e., heads or tails) in advance. And there is an element of chance, since the outcome is uncertain.
\

**The Sample Space**
\

- A sample space is a set of elements that represents all possible outcomes of a statistical experiment.\

- A sample point is an element of a sample space.\

- An event is a subset of a sample space - one or more sample points.\
\
\

<h2> 1.2. Types of Probability Distribution</h2>
There are two types of probability distribution which are used for different purposes and various types of the data generation process.

- Normal or Cumulative Probability Distribution
\
- Binomial or Discrete Probability Distribution
\

Let us discuss now both the types along with its definition, formula and examples.

<h2> 1.3. Cumulative Probability Distributions</h2> 
\

A cumulative probability refers to the probability that the value of a random variable falls within a specified range.

Let us return to the coin flip experiment. If we flip a coin two times, we might ask: What is the probability that the coin flips would result in one or fewer heads? The answer would be a cumulative probability. It would be the probability that the coin flip experiment results in zero heads plus the probability that the experiment results in one head.
\

**P(X < 1) = P(X = 0) + P(X = 1) = 0.25 + 0.50 = 0.75**

Like a probability distribution, a cumulative probability distribution can be represented by a table or an equation. In the table below, the cumulative probability refers to the probability than the random variable X is less than or equal to x.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table3.JPG)

\
\

The cumulative probability distribution is also known as a continuous probability distribution. In this distribution, the set of possible outcomes can take on values on a continuous range.

For example, a set of real numbers, is a continuous or normal distribution, as it gives all the possible outcomes of real numbers. Similarly, set of complex numbers, set of prime numbers, set of whole numbers etc. are the examples of Normal Probability distribution. Also, in real-life scenarios, the temperature of the day is an example of continuous probability. Based on these outcomes we can create a distribution table. A probability density function describes it. The formula for the normal distribution is;
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form1.png)
\
\
Where,

     μ = Mean Value
     σ = Standard Distribution of probability.
     If mean(μ) = 0 and standard deviation(σ) = 1, then this distribution is known to be normal distribution.
     x = Normal random variable
     
     
\
<h3>1.3.1.  Normal Distribution Examples</h3>
\
\
Since the normal distribution statistics estimates many natural events so well, it has evolved into a standard of recommendation for many probability queries. Some of the examples are:

- Height of the Population of the world
\
- Rolling a dice (once or multiple times)
\
- To judge Intelligent Quotient Level of children in this competitive world
\
- Tossing a coin
\
- Income distribution in countries economy among poor and rich
\
- The sizes of females shoes
\
- Weight of newly born babies range
\
- Average report of Students based on their performance
\
\
<h2>1.4.  Binomial or Discrete Probability Distribution </h2> 
\
A distribution is called a discrete probability distribution, where the set of outcomes are discrete in nature.

For example, if a dice is rolled, then all the possible outcomes are discrete and give a mass of outcomes. This is also known as probability mass functions.

So, the outcomes of binomial distribution consist of n repeated trials and the outcome may or may not occur. The formula for the binomial distribution is;
\

![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form2.png)
\
\
Where,

       n = Total number of events
       r = Total number of successful events.
       p = Success on a single trial probability.
       nCr = [n!/r!(n−r)]!
       1 – p = Failure Probabilit
\
\


<h3>1.4.1. Binomial Distribution Examples</h3> 
\
As we already know, binomial distribution gives the possibility of a different set of outcomes. In the real-life, the concept is used for:
\

- To find the number of used and unused materials while manufacturing a product.
\

- To take a survey of positive and negative feedback from the people for anything.\
\

- To check if a particular channel is watched by how many viewers by calculating the survey of YES/NO.\
\

- The number of men and women working in a company.\
\

- To count the votes for a candidate in an election and many more.\
\
\
<h3>1.4.2 What is Negative Binomial Distribution?</h3> 
\
\
In probability theory and statistics, if in a discrete probability distribution, the number of successes in a series of independent and identically disseminated Bernoulli trials before a particularised number of failures happens, then it is termed as the negative binomial distribution. Here the number of failures is denoted by ‘r’. For instance, if we throw a dice and determine the occurrence of 1 as a failure and all non-1’s as successes. Now, if we throw a dice frequently until 1 appears the third time, i.e.r = three failures, then the probability distribution of the number of non-1s that arrived would be the negative binomial distribution.
\
\
<h1> **2. Poisson Probability Distribution** </h1> 
\
The Poisson probability distribution is a discrete probability distribution that represents the probability of a given number of events happening in a fixed time or space if these cases occur with a known steady rate and individually of the time since the last event. It was titled after French mathematician Siméon Denis Poisson. The Poisson distribution can also be practised for the number of events happening in other particularised intervals such as distance, area or volume. Some of the real-life examples are:

- A number of patients arriving at a clinic between 10 to 11 AM.
\
- The number of emails received by a manager between the office hours.
\
- The number of apples sold by a shopkeeper in the time period of 12 pm to 4 pm daily.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table4.JPG)
\
\

<h2> 2.1. Probability Distribution Function</h2> 
\
A function which is used to define the distribution of a probability is called a Probability distribution function. Depending upon the types, we can define these functions. Also, these functions are used in terms of probability density functions for any given random variable.

In the case of Normal distribution,  the function of a real-valued random variable X is the function given by;

**FX(x) = P(X ≤ x)**
\
Where P shows the probability that the random variable X occurs on less than or equal to the value of x.
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\graph1.png)

\
For a closed interval, (a→b), the cumulative probability function can be defined as;

**P(a<X ≤ b) = FX(b) – FX(a)**

If we express, the cumulative probability function as integral of its probability density function fX , then,
\

\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form3.png)
\
In the case of a random variable X=b, we can define cumulative probability function as;

\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form4.png)

\
**In the case of Binomial distribution,** as we know it is defined as the probability of mass or discrete random variable gives exactly some value. This distribution is also called probability mass distribution and the function associated with it is called a probability mass function.

Probability mass function is basically defined for scalar or multivariate random variables whose domain is variant or discrete. Let us discuss its formula:

Suppose a random variable X and sample space S is defined as;
\
**X : S → A**

And A ∈ R, where R is a discrete random variable.

Then the probability mass function fX : A → [0,1] for X can be defined as;

**fX(x) = Pr (X=x) = P ({s ∈ S : X(s) = x})**
\
\
<h2> 2.2. Probability Distribution Table</h2> 
\
\
The table could be created based on the random variable and possible outcomes. Say, a random variable X is a real-valued function whose domain is the sample space of a random experiment. The probability distribution P(X) of a random variable X is the system of numbers.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table6.JPG)

\
where Pi > 0, i=1 to n and P1+P2+P3+ …….. +Pn =1
\

<h2>2.3. What is the Prior Probability?</h2>
\
In Bayesian statistical conclusion, a prior probability distribution, also known as the prior, of an unpredictable quantity is the probability distribution, expressing one’s faiths about this quantity before any proof is taken into the record. For instance, the prior probability distribution represents the relative proportions of voters who will vote for some politician in a forthcoming election. The hidden quantity may be a parameter of the design or a possible variable rather than a perceptible variable.
\
\
<h2>2.4.  What is Posterior Probability?</h2> 
\

The posterior probability is the likelihood an event will occur after all data or background information has been brought into account. It is nearly associated with a prior probability, where an event will occur before you take any new data or evidence into consideration. It is an adjustment of prior probability. We can calculate it by using the below formula:

**Posterior Probability = Prior Probability + New Evidence**

It is commonly used in Bayesian hypothesis testing. For instance, old data propose that around 60% of students who begin college will graduate within 4 years. This is the prior probability. Still, if we think the figure is much lower, so we start collecting new data. The data collected implies that the true figure is closer to 50%, which is the posterior probability.
\
\
<h2>2.5. Solved Examples</h2> 
\

**Example 1:**

A coin is tossed twice. X is the random variable of the number of heads obtained. What is the probability distribution of x?

**Solution:**

First write, the value of X= 0, 1 and 2, as the possibility are there that

No head comes

One head and one tail comes

And head comes in both the coins

Now the probability distribution could be written as;

P(X=0) = P(Tail+Tail) = ½ * ½ = ¼

P(X=1) = P(Head+Tail) or P(Tail+Head) = ½ * ½ + ½ *½ = ½

P(X=2) = P(Head+Head) = ½ * ½ = ¼

We can put these values in tabular form;
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table6.JPG)

\
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table7.JPG)
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table8.JPG)
\
\
<h2> 2.6. Some Question</h2>
\
<h3> 2.6.1. What is the probability distribution used for?</h3> 
\
\
The probability distribution is one of the important concepts in statistics. It has huge applications in business, engineering, medicine and other major sectors. It is majorly used to make future predictions based on a sample for a random experiment.
For example, in business, it is used to predict if there will be profit or loss to the company using any new strategy or by proving any hypothesis test in the medical field, etc.
\
\
<h3> 2.6.2. What is the importance of Probability distribution in Statistics?</h3> 
\
In statistics, to estimate the probability of a certain event to occur or estimate the change in occurrence and random phenomena modelled based on the distribution. Statisticians take a sample of the population to estimate the probability of occurrence of an event.
\
\
<h3> 2.6.3. What are the conditions of the Probability distribution?</h3> 
\
\
 **There are basically two major conditions:**
 
- The probabilities for random events must lie between 0 to 1.\

- The sum of all the probabilities of outcomes should be equal to 1.\
\

\
\
 <h1> **3. Probablity Distributions** </h1> 
 \
 \
 <h2> 3.1. Probability Distributions: Discrete vs. Continuous </h2> 
\
\
All probability distributions can be classified as discrete probability distributions or as continuous probability distributions, depending on whether they define probabilities associated with discrete variables or continuous variables.
\

<h3> 3.1.2. Discrete vs. Continuous Variables</h3>
\
\
If a variable can take on any value between two specified values, it is called a continuous variable; otherwise, it is called a discrete variable.

Some examples will clarify the difference between discrete and continuous variables -
\
\

- Suppose the fire department mandates that all fire fighters must weigh between 150 and 250 pounds. The weight of a fire fighter would be an example of a continuous variable; since a fire fighter's weight could take on any value between 150 and 250 pounds.
\

- Suppose we flip a coin and count the number of heads. The number of heads could be any integer value between 0 and plus infinity. However, it could not be any number between 0 and plus infinity. We could not, for example, get 2.5 heads. Therefore, the number of heads must be a discrete variable.
Just like variables, probability distributions can be classified as discrete or continuous.\

\
\
<h3>3.1.3.  Discrete Probability Distributions</h3> 
\
\
If a random variable is a discrete variable, its probability distribution is called a discrete probability distribution.

An example will make this clear. Suppose you flip a coin two times. This simple statistical experiment can have four possible outcomes: **HH, HT, TH, and TT.** Now, let the random variable X represent the number of Heads that result from this experiment. The random variable X can only take on the values 0, 1, or 2, so it is a discrete random variable.

The probability distribution for this statistical experiment appears below.
\
\

![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table9.JPG)
\
\
The above table represents a discrete probability distribution because it relates each value of a discrete random variable with its probability of occurrence. On this website, we will cover the following discrete probability distributions.
\
\

- Binomial probability distribution
\

- Hypergeometric probability distribution
\

- Multinomial probability distribution
\

- Negative binomial distribution
\

- Poisson probability distribution
\

\
\
<h2>3.2. Binomial Experiment </h2> 
\
\
A binomial experiment is a statistical experiment that has the following properties:

- The experiment consists of n repeated trials.
\

- Each trial can result in just two possible outcomes. We call one of these outcomes a success and the other, a failure.
\

- The probability of success, denoted by P, is the same on every trial.
\
- The trials are independent; that is, the outcome on one trial does not affect the outcome on other trials.
\
\

Consider the following statistical experiment. You flip a coin 2 times and count the number of times the coin lands on heads. This is a binomial experiment because:
\

- The experiment consists of repeated trials. We flip a coin 2 times.
\
- Each trial can result in just two possible outcomes - heads or tails.
\
- The probability of success is constant - 0.5 on every trial.
\
- The trials are independent; that is, getting heads on one trial does not affect whether we get heads on other trials.
\
\
**Notation** 
\
\
The following notation is helpful, when we talk about binomial probability.
\
\
- x: The number of successes that result from the binomial experiment.
\
- n: The number of trials in the binomial experiment.
\
- P: The probability of success on an individual trial.
\
- Q: The probability of failure on an individual trial. (This is equal to 1 - P.)
\
- n!: The factorial of n (also known as n factorial).
\
- b(x; n, P): Binomial probability - the probability that an n-trial binomial experiment results in exactly x successes, when the probability of success on an individual trial is P.
\
- nCr: The number of combinations of n things, taken r at a time.
\
\

<h3>3.2.1.  Binomial Distribution </h3> 
\
\
A binomial random variable is the number of successes x in n repeated trials of a binomial experiment. The probability distribution of a binomial random variable is called a binomial distribution.

Suppose we flip a coin two times and count the number of heads (successes). The binomial random variable is the number of heads, which can take on values of 0, 1, or 2. The binomial distribution is presented below.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table10.JPG)

\
\
The binomial distribution has the following properties:

- The mean of the distribution (μx) is equal to n * P .
\
- The variance (σ2x) is n * P * ( 1 - P ).
\
- The standard deviation (σx) is sqrt[ n * P * ( 1 - P ) ].
\
\
Given x, n, and P, we can compute the binomial probability based on the binomial formula:

Binomial Formula. Suppose a binomial experiment consists of n trials and results in x successes. If the probability of success on an individual trial is P, then the binomial probability is:
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form5.JPG)
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\xmpl1.JPG)

\

<h3>3.2.2 Cumulative Binomial Probability </h3> 

A cumulative binomial probability refers to the probability that the binomial random variable falls within a specified range (e.g., is greater than or equal to a stated lower limit and less than or equal to a stated upper limit).

For example, we might be interested in the cumulative binomial probability of obtaining 45 or fewer heads in 100 tosses of a coin (see Example 1 below). This would be the sum of all these individual binomial probabilities.

b(x < 45; 100, 0.5) =
b(x = 0; 100, 0.5) + b(x = 1; 100, 0.5) + ... + b(x = 44; 100, 0.5) + b(x = 45; 100, 0.5)
\
\

**Example 2**
\
\

- What is the probability of obtaining 45 or fewer heads in 100 tosses of a coin?

Solution: To solve this problem, we compute 46 individual probabilities, using the binomial formula. The sum of all these probabilities is the answer we seek. Thus,

b(x < 45; 100, 0.5) = b(x = 0; 100, 0.5) + b(x = 1; 100, 0.5) + . . . + b(x = 45; 100, 0.5)
\

b(x < 45; 100, 0.5) = 0.184
\
\

**Example 3**

The probability that a student is accepted to a prestigious college is 0.3. If 5 students from the same school apply, what is the probability that at most 2 are accepted?

**Solution:** To solve this problem, we compute 3 individual probabilities, using the binomial formula. The sum of all these probabilities is the answer we seek. Thus,

b(x < 2; 5, 0.3) = b(x = 0; 5, 0.3) + b(x = 1; 5, 0.3) + b(x = 2; 5, 0.3)\
\
b(x < 2; 5, 0.3) = 0.1681 + 0.3601 + 0.3087\
\
b(x < 2; 5, 0.3) = 0.8369\
\
\
\

**Example 4**

- What is the probability that the world series will last 4 games? 5 games? 6 games? 7 games? Assume that the teams are evenly matched.
\

**Solution:** This is a very tricky application of the binomial distribution. If you can follow the logic of this solution, you have a good understanding of the material covered in the tutorial, to this point.
\
\
In the world series, there are two baseball teams. The series ends when the winning team wins 4 games. Therefore, we define a success as a win by the team that ultimately becomes the world series champion.
\
\
For the purpose of this analysis, we assume that the teams are evenly matched. Therefore, the probability that a particular team wins a particular game is 0.5.
\
\
Let's look first at the simplest case. What is the probability that the series lasts only 4 games. This can occur if one team wins the first 4 games. The probability of the National League team winning 4 games in a row is:
\
\

b(4; 4, 0.5) = 4C4 * (0.5)4 * (0.5)0 = 0.0625
\
\

Similarly, when we compute the probability of the American League team winning 4 games in a row, we find that it is also **0.0625.** 

\
Therefore, probability that the series ends in four games would be **0.0625 + 0.0625 = 0.125;**  since the series would end if either the American or National League team won 4 games in a row.
\
\
Now let's tackle the question of finding probability that the world series ends in 5 games. The trick in finding this solution is to recognize that the series can only end in 5 games, if one team has won 3 out of the first 4 games. So let's first find the probability that the American League team wins exactly 3 of the first 4 games.
\

**b(3; 4, 0.5) = 4C3 * (0.5)3 * (0.5)1 = 0.25**
\
\
Okay, here comes some more tricky stuff, so listen up. Given that the American League team has won 3 of the first 4 games, the American League team has a 50/50 chance of winning the fifth game to end the series. Therefore, the probability of the American League team winning the series in 5 games is **0.25 * 0.50 = 0.125.** Since the National League team could also win the series in 5 games, the probability that the series ends in 5 games would be **0.125 + 0.125 = 0.25.**
\
\
The rest of the problem would be solved in the same way. You should find that the probability of the series ending in 6 games is 0.3125; and the probability of the series ending in 7 games is also 0.3125.
\
\

<h2>3.3. Hypergeometric Distribution</h2>

\
\
The probability distribution of a hypergeometric random variable is called a hypergeometric distribution. This lesson describes how hypergeometric random variables, hypergeometric experiments, hypergeometric probability, and the hypergeometric distribution are all related.
\
\
**Notation**
\

The following notation is helpful, when we talk about hypergeometric distributions and hypergeometric probability.
\

- N: The number of items in the population.
\

- k: The number of items in the population that are classified as successes.
\

- n: The number of items in the sample.
\

- x: The number of items in the sample that are classified as successes.
\

- kCx: The number of combinations of k things, taken x at a time.
\

- h(x; N, n, k): hypergeometric probability - the probability that an n-trial hypergeometric experiment results in exactly x successes, when the population consists of N items, k of which are classified as successes.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\text.JPG)
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\xmpl2.JPG)
\
\
<h3> 3.3.1. Cumulative Hypergeometric Probability</h2> 
\
\
\

A cumulative hypergeometric probability refers to the probability that the hypergeometric random variable is greater than or equal to some specified lower limit and less than or equal to some specified upper limit.

For example, suppose we randomly select five cards from an ordinary deck of playing cards. We might be interested in the cumulative hypergeometric probability of obtaining 2 or fewer hearts. This would be the probability of obtaining 0 hearts plus the probability of obtaining 1 heart plus the probability of obtaining 2 hearts, as shown in the example below.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\xmpl3.JPG)
\
\
\
<h2>3.4. Multinomial Distribution</h2>
\
\
A multinomial distribution is the probability distribution of the outcomes from a multinomial experiment.
\
\
<h3>3.4.1 Multinomial Experiment</h3>
\
\
A multinomial experiment is a statistical experiment that has the following properties:

\
\

- The experiment consists of n repeated trials.
\

- Each trial has a discrete number of possible outcomes.
\

- On any given trial, the probability that a particular outcome will occur is constant.
\

- The trials are independent; that is, the outcome on one trial does not affect the outcome on other trials.
\

Consider the following statistical experiment. You toss two dice three times, and record the outcome on each toss. This is a multinomial experiment because:

\
\

- The experiment consists of repeated trials. We toss the dice three times.
\

- Each trial can result in a discrete number of outcomes - 2 through 12.
\

- The probability of any outcome is constant; it does not change from one toss to the next.
\

- The trials are independent; that is, getting a particular outcome on one trial does not affect the outcome on other trials\
\
\
**Note:** A binomial experiment is a special case of a multinomial experiment. Here is the main difference. With a binomial experiment, each trial can result in two - and only two - possible outcomes. With a multinomial experiment, each trial can have two or more possible outcomes.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\text2.JPG)

\
\
**Test Your Understanding**
\
\
**Problem 1**

- Suppose a card is drawn randomly from an ordinary deck of playing cards, and then put back in the deck. This exercise is repeated five times. What is the probability of drawing 1 spade, 1 heart, 1 diamond, and 2 clubs?
\
\
**Solution:** To solve this problem, we apply the multinomial formula. We know the following:

The experiment consists of 5 trials, so n = 5.
\

The 5 trials produce 1 spade, 1 heart, 1 diamond, and 2 clubs;
\
so n1 = 1, n2 = 1, n3 = 1, and n4 = 2.\
\

On any particular trial, the probability of drawing a spade, heart, diamond, or club is 0.25, 0.25, 0.25, and 0.25, respectively. 
\
\
Thus, p1 = 0.25, p2 = 0.25, p3 = 0.25, and p4 = 0.25.
\

We plug these inputs into the multinomial formula, as shown below:
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form6.JPG)
\
\
\

![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\text3.JPG)

\
\
\
<h2>3.5. Negative Binomial Distribution</h2>
\
\
In this lesson, we cover the negative binomial distribution and the geometric distribution. As we will see, the geometric distribution is a special case of the negative binomial distribution.
\
\
<h3>3.5.1. Negative Binomial Experiment</h3>
\
\
\
A negative binomial experiment is a statistical experiment that has the following properties:
\
\

- The experiment consists of x repeated trials.
\

- Each trial can result in just two possible outcomes. We call one of these outcomes a success and the other, a failure.
\

- The probability of success, denoted by P, is the same on every trial.
\

- The trials are independent; that is, the outcome on one trial does not affect the outcome on other trials.
\

- The experiment continues until r successes are observed, where r is specified in advance.
\
\
Consider the following statistical experiment. You flip a coin repeatedly and count the number of times the coin lands on heads. You continue flipping the coin until it has landed 5 times on heads. This is a negative binomial experiment because:
\
\
- The experiment consists of repeated trials. We flip a coin repeatedly until it has landed 5 times on heads.
\

- Each trial can result in just two possible outcomes - heads or tails.
\

- The probability of success is constant - 0.5 on every trial.
\

- The trials are independent; that is, getting heads on one trial does not affect whether we get heads on other trials.
\

- The experiment continues until a fixed number of successes have occurred; in this case, 5 heads.
\
\
\
**Notation**
\
\
The following notation is helpful, when we talk about negative binomial probability.
\
\
- x: The number of trials required to produce r successes in a negative binomial experiment.
\

- r: The number of successes in the negative binomial experiment.
\

- P: The probability of success on an individual trial.
\

- Q: The probability of failure on an individual trial. (This is equal to 1 - P.)
\

- b*(x; r, P): Negative binomial probability - the probability that an x-trial negative binomial experiment results in the rth success on the xth trial, when the probability of success on an individual trial is P.
\

- nCr: The number of combinations of n things, taken r at a time.
\
\
\
<h3>3.5.2. Negative Binomial Distribution</h3>
\
\
A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution.
\
\
Suppose we flip a coin repeatedly and count the number of heads (successes). If we continue flipping the coin until it has landed 2 times on heads, we are conducting a negative binomial experiment. \
The negative binomial random variable is the number of coin flips required to achieve 2 heads. In this example, the number of coin flips is a random variable that can take on any integer value between 2 and plus infinity. The negative binomial probability distribution for this example is presented below.
\
\

\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\table11.JPG)
\
\
\
<h3>3.5.3. Negative Binomial Probability</h3>

The negative binomial probability refers to the probability that a negative binomial experiment results in r - 1 successes after trial x - 1 and r successes after trial x. For example, in the above table, we see that the negative binomial probability of getting the second head on the sixth flip of the coin is 0.078125.
\

Given x, r, and P, we can compute the negative binomial probability based on the following formula:

\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form7.JPG)

\
\
\
<h3>3.5.4. The Mean of the Negative Binomial Distribution</h3>

\
\
If we define the mean of the negative binomial distribution as the average number of trials required to produce r successes, then the mean is equal to:
\

μ = r / P
\

where μ is the mean number of trials, r is the number of successes, and P is the probability of a success on any given trial.
\
\
\
**The moral: If someone talks about a negative binomial distribution, find out how they are defining the negative binomial random variable.**
\
\
\
<h3>3.5.5.  Geometric Distribution</h3>
\
\
The geometric distribution is a special case of the negative binomial distribution. It deals with the number of trials required for a single success. Thus, the geometric distribution is negative binomial distribution where the number of successes (r) is equal to 1.
\

An example of a geometric distribution would be tossing a coin until it lands on heads. We might ask: What is the probability that the first head occurs on the third flip? That probability is referred to as a geometric probability and is denoted by g(x; P). The formula for geometric probability is given below.
\
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form8.JPG)
\
\
\
<h3>3.5.6.Test Your Understanding</h3>
\
\
\
**Example 1**
\

- Bob is a high school basketball player. He is a 70% free throw shooter. That means his probability of making a free throw is 0.70. During the season, what is the probability that Bob makes his third free throw on his fifth shot?

**Solution:** This is an example of a negative binomial experiment. The probability of success (P) is 0.70, the number of trials (x) is 5, and the number of successes (r) is 3.
\
To solve this problem, we enter these values into the negative binomial formula.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form9.JPG)
\
\
Thus, the probability that Bob will make his third successful free throw on his fifth shot is 0.18522.
\
\
\
**Example 2 ** 
\

- Let's reconsider the above problem from Example 1. This time, we'll ask a slightly different question: What is the probability that Bob makes his first free throw on his fifth shot?

**Solution:** This is an example of a geometric distribution, which is a special case of a negative binomial distribution. Therefore, this problem can be solved using the negative binomial formula or the geometric formula. We demonstrate each approach below, beginning with the negative binomial formula.
\

The probability of success (P) is 0.70, the number of trials (x) is 5, and the number of successes (r) is 1. We enter these values into the negative binomial formula.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form10.JPG)

\
\
\
<h3>3.6. Uniform Probability Distribution</h3>

\
The simplest probability distribution occurs when all of the values of a random variable occur with equal probability. This probability distribution is called the uniform distribution.
\
Uniform Distribution. Suppose the random variable X can assume k different values. Suppose also that the P(X = xk) is constant. Then,

                                              P(X = xk) = 1/k
\
**Example 1**

- Suppose a die is tossed. What is the probability that the die will land on 5?

**Solution:** \
When a die is tossed, there are 6 possible outcomes represented by: S = { 1, 2, 3, 4, 5, 6 }. 
\
Each possible outcome is a random variable (X), and each outcome is equally likely to occur.
\
Thus, we have a uniform distribution. Therefore, the P(X = 5) = 1/6.
\

**Example 2**

- Suppose we repeat the dice tossing experiment described in Example 1. This time, we ask what is the probability that the die will land on a number that is smaller than 5?

**Solution:** 
When a die is tossed, there are 6 possible outcomes represented by: S = { 1, 2, 3, 4, 5, 6 }. 
Each possible outcome is equally likely to occur. Thus, we have a uniform distribution.

This problem involves a cumulative probability. 
The probability that the die will land on a number smaller than 5 is equal to:
\

P( X < 5 ) = P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4)
\
P( X < 5 ) = 1/6 + 1/6 + 1/6 + 1/6 = 2/3
\
\
\
\
\
<h1>**4. Continuous Probability Distributions**</h1> 
\
\
If a random variable is a continuous variable, its probability distribution is called a continuous probability distribution.

A continuous probability distribution differs from a discrete probability distribution in several ways.
\
\

- The probability that a continuous random variable will assume a particular value is zero.
\

- As a result, a continuous probability distribution cannot be expressed in tabular form.
\

- Instead, an equation or formula is used to describe a continuous probability distribution.
\

\
\
Most often, the equation used to describe a continuous probability distribution is called a probability density function. Sometimes, it is referred to as a density function, a PDF, or a pdf. For a continuous probability distribution, the density function has the following properties:
\
\
\

- Since the continuous random variable is defined over a continuous range of values (called the domain of the variable), the graph of the density function will also be continuous over that range.
\

- The area bounded by the curve of the density function and the x-axis is equal to 1, when computed over the domain of the variable.
\

- The probability that a random variable assumes a value between a and b is equal to the area under the density function bounded by a and b.
\

\
\
For example, consider the probability density function shown in the graph below. Suppose we wanted to know the probability that the random variable X was less than or equal to a. The probability that X is less than or equal to a is equal to the area under the curve bounded by a and minus infinity - as indicated by the shaded area.
\
\

![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\graph.JPG)
\
\
\
Note: The shaded area in the graph represents the probability that the random variable X is less than or equal to a. This is a cumulative probability. However, the probability that X is exactly equal to a would be zero. A continuous random variable can take on an infinite number of values. The probability that it will equal a specific value (such as a) is always zero.
\
\
**we cover the following continuous probability distributions.**
\

- Normal probability distribution
\

- Student's t distribution
\

- Chi-square distribution
\

- F distribution
\
\
<h2> 4.1. Normal probability distribution</h2> 
\
\
The normal distribution refers to a family of continuous probability distributions described by the normal equation.
\
\
<h3>4.1.1. The Normal Equation</h3> 
\
\
The normal distribution is defined by the following equation:

\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form11.JPG)
\
\
The random variable X in the normal equation is called the normal random variable. The normal equation is the probability density function for the normal distribution.

\
\
<h3> 4.1.2. The Normal Curve</h3>
\
\
The graph of the normal distribution depends on two factors - the mean and the standard deviation. The mean of the distribution determines the location of the center of the graph, and the standard deviation determines the height and width of the graph.
\
All normal distributions look like a symmetric, bell-shaped curve, as shown below.
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\graph1.JPG)

\
\
When the standard deviation is small, the curve is tall and narrow; and when the standard deviation is big, the curve is short and wide (see above)
\
\
<h3> 4.1.3. Probability and the Normal Curve</h3> 
\
\
The normal distribution is a continuous probability distribution. This has several implications for probability.
\
\

- The total area under the normal curve is equal to 1.
\

- The probability that a normal random variable X equals any particular value is 0.
\

- The probability that X is greater than a equals the area under the normal curve bounded by a and plus infinity (as indicated by the non-shaded area in the figure below).
\

- The probability that X is less than a equals the area under the normal curve bounded by a and minus infinity (as indicated by the shaded area in the figure below).
\
\
\
![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\graph2.JPG)

\
\
Additionally, every normal curve (regardless of its mean or standard deviation) conforms to the following "rule".
\
\

- About 68% of the area under the curve falls within 1 standard deviation of the mean.
\

- About 95% of the area under the curve falls within 2 standard deviations of the mean.
\

- About 99.7% of the area under the curve falls within 3 standard deviations of the mean
\
\
Collectively, these points are known as the empirical rule or the 68-95-99.7 rule. Clearly, given a normal distribution, most outcomes will be within 3 standard deviations of the mean.

\
\
<h3> 4.1.4. Exercise </h3> 
\
\
\
**Problem 1**
\

- An average light bulb manufactured by the Acme Corporation lasts 300 days with a standard deviation of 50 days. Assuming that bulb life is normally distributed, what is the probability that an Acme light bulb will last at most 365 days?

\
\

**Solution:** Given a mean score of 300 days and a standard deviation of 50 days, we want to find the cumulative probability that bulb life is less than or equal to 365 days. Thus, we know the following:
\
\

- The value of the normal random variable is 365 days.
\

- The mean is equal to 300 days.
\

- The standard deviation is equal to 50 days.
\

- We enter these values into the Normal Distribution Calculator and compute the cumulative probability. 
\
The answer is: P( X < 365) = 0.90. Hence, there is a 90% chance that a light bulb will burn out within 365 days.
\
\
\

**Problem 2**

- Suppose scores on an IQ test are normally distributed. If the test has a mean of 100 and a standard deviation of 10, what is the probability that a person who takes the test will score between 90 and 110?
\
\

**Solution:** Here, we want to know the probability that the test score falls between 90 and 110. The "trick" to solving this problem is to realize the following:
\

**P( 90 < X < 110 ) = P( X < 110 ) - P( X < 90 )**
\

We use the Normal Distribution Calculator to compute both probabilities on the right side of the above equation.
\

- To compute P( X < 110 ), we enter the following inputs into the calculator: The value of the normal random variable is 110, the mean is 100, and the standard deviation is 10. We find that P( X < 110 ) is 0.84.
\

- To compute P( X < 90 ), we enter the following inputs into the calculator: The value of the normal random variable is 90, the mean is 100, and the standard deviation is 10. We find that P( X < 90 ) is 0.16.
\

We use these findings to compute our final answer as follows:
\
P( 90 < X < 110 ) = P( X < 110 ) - P( X < 90 )
\

P( 90 < X < 110 ) = 0.84 - 0.16
\

P( 90 < X < 110 ) = 0.68
\


Thus, about 68% of the test scores will fall between 90 and 110.
\
\
\
\
<h2> 4.2. Student's t Distribution </h2>
\
\
The t distribution (aka, Student’s t-distribution) is a probability distribution that is used to estimate population parameters when the sample size is small and/or when the population variance is unknown.
\
\

<h3>4.2.1.  Why Use the t Distribution?</h3>
\
\
According to the central limit theorem, the sampling distribution of a statistic (like a sample mean) will follow a normal distribution, as long as the sample size is sufficiently large. Therefore, when we know the standard deviation of the population, we can compute a z-score, and use the normal distribution to evaluate probabilities with the sample mean.
\

But sample sizes are sometimes small, and often we do not know the standard deviation of the population. When either of these problems occur, statisticians rely on the distribution of the t statistic (also known as the t score), whose values are given by:

\
\
**t = [ x' - μ ] / [ s / sqrt( n ) ]**
\
\

where x' is the sample mean, μ is the population mean, s is the standard deviation of the sample, and n is the sample size. The distribution of the t statistic is called the t distribution or the Student t distribution.
\
The t distribution allows us to conduct statistical analyses on certain data sets that are not appropriate for analysis, using the normal distribution.
\
\
<h3> 4.2.2. Degrees of Freedom</h3> 
\
\
There are actually many different t distributions. The particular form of the t distribution is determined by its degrees of freedom. The degrees of freedom refers to the number of independent observations in a set of data.
\

When estimating a mean score or a proportion from a single sample, the number of independent observations is equal to the sample size minus one. Hence, the distribution of the t statistic from samples of size 8 would be described by a t distribution having 8 - 1 or 7 degrees of freedom. Similarly, a t distribution having 15 degrees of freedom would be used with a sample of size 16.
\

For other applications, the degrees of freedom may be calculated differently. We will describe those computations as they come up.
\
\
<h3> 4.2.3. Properties of the t Distribution</h3>
\
\
\
The t distribution has the following properties:
\

- The mean of the distribution is equal to 0 .
\

- The variance is equal to v / ( v - 2 ), where v is the degrees of freedom (see last section) and v > 2.
\

- The variance is always greater than 1, although it is close to 1 when there are many degrees of freedom. With infinite degrees of freedom, the t distribution is the same as the standard normal distribution.
\
\
<h3> 4.2.4. When to Use the t Distribution </h3> 
\
\
The t distribution can be used with any statistic having a bell-shaped distribution (i.e., approximately normal). The sampling distribution of a statistic should be bell-shaped if any of the following conditions apply.
\
\

- The population distribution is normal.
\

- The population distribution is symmetric, unimodal, without outliers, and the sample size is at least 30.
\

- The population distribution is moderately skewed, unimodal, without outliers, and the sample size is at least 40.
\

- The sample size is greater than 40, without outliers.
\
\
The t distribution should not be used with small samples from populations that are not approximately normal.

\
\
<h3> 4.2.5. Probability and the Student t Distribution </h5>
\
\
When a sample of size n is drawn from a population having a normal (or nearly normal) distribution, the sample mean can be transformed into a t statistic, using the equation presented at the beginning of this lesson. We repeat that equation below:
\
\
**t = [ x' - μ ] / [ s / sqrt( n ) ]**
\
\
where x' is the sample mean, μ is the population mean, s is the standard deviation of the sample, n is the sample size, and degrees of freedom are equal to n - 1.

The t statistic produced by this transformation can be associated with a unique cumulative probability. This cumulative probability represents the likelihood of finding a sample mean less than or equal to x, given a random sample of size n.
\
\
<h3> 4.2.6. Exercise </h3> 
\

**Problem 1**
\

- Acme Corporation manufactures light bulbs. The CEO claims that an average Acme light bulb lasts 300 days. A researcher randomly selects 15 bulbs for testing. The sampled bulbs last an average of 290 days, with a standard deviation of 50 days. If the CEO's claim were true, what is the probability that 15 randomly selected bulbs would have an average life of no more than 290 days?

\
\

**Solution: **
\
\
The first thing we need to do is compute the t statistic, based on the following equation:

t = [ x' - μ ] / [ s / sqrt( n ) ]
t = ( 290 - 300 ) / [ 50 / sqrt( 15) ]
t = -10 / 12.909945 = - 0.7745966

where - 
                                            x' is the sample mean,
                                           μ is the population mean,
                                          s is the standard deviation of the sample, 
                                        and n is the sample size.
\
\
\
The cumulative probability: 0.226. Hence, if the true bulb life were 300 days, there is a 22.6% chance that the average bulb life for 15 randomly selected bulbs would be less than or equal to 290 days.
\
\

**Problem 2**
\
\

- Suppose scores on an IQ test are normally distributed, with a population mean of 100. Suppose 20 people are randomly selected and tested. The standard deviation in the sample group is 15. What is the probability that the average test score in the sample group will be at most 110?
\
\

**Solution:**
\
\

To solve this problem, we will work directly with the raw data from the problem. We will not compute the t statistic; the T Distribution Calculator will do that work for us. Since we will work with the raw data, we select "Sample mean" from the Random Variable dropdown box. Then, we enter the following data:

- The degrees of freedom are equal to 20 - 1 = 19.
\

- The population mean equals 100.
\

- The sample mean equals 110.
\

- The standard deviation of the sample is 15.
\
\

 The cumulative probability: 0.996. Hence, there is a 99.6% chance that the sample average will be no greater than 110.
\
\

\
\

<h2> 4.3. Chi-Square Distribution </h2> 
\
\

The distribution of the chi-square statistic is called the chi-square distribution. In this lesson, we learn to compute the chi-square statistic and find the probability associated with the statistic. And we'll work through some chi-square examples to illustrate key points.

\
\
<h3> 4.3.1. The Chi-Square Statistic</h3>
\
\
Suppose we conduct the following statistical experiment. We select a random sample of size n from a normal population, having a standard deviation equal to σ. We find that the standard deviation in our sample is equal to s. Given these data, we can define a statistic, called chi-square, using the following equation:

\
\
$$
\begin{align}
Χ^2 = [ ( n - 1 ) * s^2 ] / σ^2
\end{align}\
$$

\
\
The distribution of the chi-square statistic is called the chi-square distribution. The chi-square distribution is defined by the following probability density function:
\
\

![](G:\DATA SCIENCE\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\form12.JPG)


\
\
where Y0 is a constant that depends on the number of degrees of freedom, Χ2 is the chi-square statistic, v = n - 1 is the number of degrees of freedom, and e is a constant equal to the base of the natural logarithm system (approximately 2.71828). 
\
Y0 is defined, so that the area under the chi-square curve is equal to one.
\
\
In the figure below, the red curve shows the distribution of chi-square values computed from all possible samples of size 3, where degrees of freedom is n - 1 = 3 - 1 = 2. Similarly, the green curve shows the distribution for samples of size 5 (degrees of freedom equal to 4); and the blue curve, for samples of size 11 (degrees of freedom equal to 10).
\
![](G:\DATA SCIENCE\\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\graph3.JPG)
\
\
\

The chi-square distribution has the following properties:
\
\

- The mean of the distribution is equal to the number of degrees of freedom: μ = v.
\

- The variance is equal to two times the number of degrees of freedom: σ2 = 2 * v
\

- When the degrees of freedom are greater than or equal to 2, the maximum value for Y occurs when Χ2 = v - 2.
\

- As the degrees of freedom increase, the chi-square curve approaches a normal distribution.
\

\
\
<h3> 4.3.2. Cumulative Probability and the Chi-Square Distribution </h3>
\
\

The chi-square distribution is constructed so that the total area under the curve is equal to 1. 
The area under the curve between 0 and a particular chi-square value is a cumulative probability associated with that chi-square value. 
\

For example, in the figure below, the shaded area represents a cumulative probability associated with a chi-square statistic equal to A; that is, it is the probability that the value of a chi-square statistic will fall between 0 and A.
\
\

![](G:\DATA SCIENCE\\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\graph4.JPG)
\
\
Fortunately, we don't have to compute the area under the curve to find the probability. The easiest way to find the cumulative probability associated with a particular chi-square statistic is to use the Chi-Square Calculator.
\
\
<h3> 4.3.3. Exercise </h3> 
\
\
\
**Problem 1 **
\

The Acme Battery Company has developed a new cell phone battery. On average, the battery lasts 60 minutes on a single charge. The standard deviation is 4 minutes.

Suppose the manufacturing department runs a quality control test. They randomly select 7 batteries. The standard deviation of the selected batteries is 6 minutes. What would be the chi-square statistic represented by this test?
\

**Solution**
\

We know the following:
\
\

- The standard deviation of the population is 4 minutes.
\

- The standard deviation of the sample is 6 minutes.
\

- The number of sample observations is 7.
\

- To compute the chi-square statistic, we plug these data in the chi-square equation, as shown below.
\
\

$$
\begin{align}
Χ2 = [ ( n - 1 ) * s^2 ] / σ^2
Χ2 = [ ( 7 - 1 ) * 6^2 ] / 4^2 = 13.5
\end{align}\
$$
\
\
where Χ^2 is the chi-square statistic, n is the sample size, s is the standard deviation of the sample, and σ is the standard deviation of the population.
\
\

\
\

<h2> 4.4. F Distribution </h2>
\
\
The F distribution is the probability distribution associated with the f statistic. In this lesson, we show how to compute an f statistic and how to find probabilities associated with specific f statistic values.
\
\
\
<h3> 4.4.1. The f Statistic </h3>
\
\
![](G:\DATA SCIENCE\\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\text4.JPG)
\
\
\
![](G:\DATA SCIENCE\\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\text5.JPG)
\
\
<h3> 4.4.2.Cumulative Probability and the F Distribution</h3> 
\
\
\
Every f statistic can be associated with a unique cumulative probability. This cumulative probability represents the likelihood that the f statistic is less than or equal to a specified value.

Statisticians use fα to represent the value of an f statistic having a cumulative probability of (1 - α). For example, suppose we were interested in the f statistic having a cumulative probability of 0.95. We would refer to that f statistic as f0.05, since (1 - 0.95) = 0.05.
\
\
\
Of course, to find the value of fα, we would need to know the degrees of freedom, v1 and v2. Notationally, the degrees of freedom appear in parentheses as follows: fα(v1,v2). Thus, f0.05(5, 7) refers to value of the f statistic having a cumulative probability of 0.95, v1 = 5 degrees of freedom, and v2 = 7 degrees of freedom.
\
\
\
\
<h3> 4.4.3. Exercise </h3> 
\
\
![](G:\DATA SCIENCE\\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\text6.JPG)
\
\
![](G:\DATA SCIENCE\\SOFTANBEES\WORKING PROCEDURE\DISTRIBUTION\images\text7.JPG)
\
\

\
\

# **Some Example  For Exercise **

\

 **Suppose I roll a die. Is that a statistical experiment?**
\

**Solution: **\
Yes. Like a coin toss, rolling dice is a statistical experiment. There is more than one possible outcome. We can specify each possible outcome in advance. And there is an element of chance.
\

 **When you roll a single die, what is the sample space.**

**Solution:**\
The sample space is all of the possible outcomes - an integer between 1 and 6.
\

 **Which of the following are sample points when you roll a die - 3, 6, and 9?**

**Solution** \
The numbers 3 and 6 are sample points, because they are in the sample space. The number 9 is not a sample point, since it is outside the sample space; with one die, the largest number that you can roll is 6.
\

 **Which of the following sets represent an event when you roll a die?**
\
A.   {1}\
B.   {2, 4,}\
C.   {2, 4, 6}\
D.   All of the above\
\

**Solution**  \
The correct answer is D. Remember that an event is a subset of a sample space. The sample space is any integer from 1 to 6. Each of the sets shown above is a subset of the sample space, so each represents an event.

 **Consider the events listed below. Which are mutually exclusive?**

A.   {1}\
B.   {2, 4,}\
C.   {2, 4, 6}\
\
\

**Solution:**  \
Two events are mutually exclusive, if they have no sample points in common. Events A and B are mutually exclusive, and Events A and C are mutually exclusive; since they have no points in common. Events B and C have common sample points, so they are not mutually exclusive.

 **Suppose you roll a die two times. Is each roll of the die an independent event?**
\

**Solution:** \
Yes. Two events are independent when the occurrence of one has no effect on the probability of the occurrence of the other. Neither roll of the die affects the outcome of the other roll; so each roll of the die is independent.
\
\
\
\
\


                                               BEST OF LUCK 


